{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f7ef3db",
   "metadata": {},
   "source": [
    "# Introduction to Unsupervised Machine Learning\n",
    "\n",
    "#### Supervised Machine Learning\n",
    "\n",
    "In **supervised learning**, we have a dataset with:\n",
    "\n",
    "* **Input features:** things we know, like age, experience, or salary\n",
    "* **Output feature:** the thing we want to predict\n",
    "\n",
    "The goal is to **predict the output** using the inputs.\n",
    "There are two main types of supervised learning:\n",
    "\n",
    "1. **Regression:** Predict numbers (e.g., salary)\n",
    "2. **Classification:** Predict categories (e.g., yes/no, red/blue)\n",
    "\n",
    "Examples of supervised learning algorithms are **Linear Regression, Logistic Regression, Decision Trees, Random Forest, and XGBoost**.\n",
    "\n",
    "---\n",
    "\n",
    "# Unsupervised Machine Learning\n",
    "\n",
    "In **unsupervised learning**, there is **no output to predict**. Instead, we try to **find patterns or group similar data together**.\n",
    "\n",
    "This is called **clustering**.\n",
    "\n",
    "**Example:**\n",
    "Imagine a dataset with **age, years of experience, and salary**.\n",
    "\n",
    "* We are **not trying to predict anything**.\n",
    "* Instead, we can **group people with similar age, experience, and salary**.\n",
    "* These groups are called **clusters**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Real-World Example: Customer Segmentation\n",
    "\n",
    "Clustering is useful for **grouping customers**.\n",
    "\n",
    "* Suppose you know customers’ **salary** and **spending score**.\n",
    "* You can make clusters like:\n",
    "\n",
    "  * Customers who buy regularly → offer a **15% discount**\n",
    "  * Customers who buy occasionally → offer a **20% discount**\n",
    "\n",
    "This helps businesses **target the right customers** without any pre-labeled data.\n",
    "\n",
    "---\n",
    "\n",
    "#### Common Unsupervised Learning Algorithms\n",
    "\n",
    "Some popular clustering algorithms are:\n",
    "\n",
    "* **K-Means Clustering**\n",
    "* **Hierarchical Clustering**\n",
    "* **DBSCAN Clustering**\n",
    "\n",
    "We can also use **silhouette scoring** to check how good the clusters are.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Points\n",
    "\n",
    "* **Supervised learning:** Predicts a known output.\n",
    "* **Unsupervised learning:** Finds patterns without an output.\n",
    "* **Clustering:** Groups similar data points.\n",
    "* **Algorithms to learn:** K-Means, Hierarchical, DBSCAN, and Silhouette scoring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6824bbcc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## **Unsupervised Learning**\n",
    "\n",
    "* **What it is:** You have data, but **no answers or labels**. The goal is to **find patterns, groups, or important features** in the data.\n",
    "* **Example:** You own a shop. You don’t tell the computer which customers are “VIP” or “regular,” but it can **group customers with similar buying habits**.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Clustering**\n",
    "\n",
    "**Goal:** Put similar items together. Think of it as **grouping friends who like the same things**.\n",
    "\n",
    "* **K-Means:**\n",
    "\n",
    "  * Pick a number of groups (K).\n",
    "  * Computer assigns each item to the nearest group.\n",
    "  * Groups adjust until things are stable.\n",
    "  * **Example:** 3 groups of fruits: sweet, sour, medium.\n",
    "\n",
    "* **Hierarchical Clustering:**\n",
    "\n",
    "  * Start with each item alone.\n",
    "  * Merge the closest ones step by step.\n",
    "  * Makes a **tree (dendrogram)** showing relationships.\n",
    "  * **Example:** Apple and Orange merge first, then Lemon joins later.\n",
    "\n",
    "* **DBSCAN:**\n",
    "\n",
    "  * Looks for areas where items are **dense** (many close together).\n",
    "  * Items far from any cluster are **outliers**.\n",
    "  * **Example:** Most people live in 2 cities → 2 clusters. Someone alone in a village → outlier.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Dimensionality Reduction**\n",
    "\n",
    "**Goal:** Simplify data while keeping the important parts. Think of it as **summarizing a big book into a short story**.\n",
    "\n",
    "* **PCA (Principal Component Analysis):**\n",
    "\n",
    "  * Combines features into a smaller number of “super-features.”\n",
    "  * Keeps most of the variation in data.\n",
    "  * **Example:** You have weight, height, age of people → PCA might combine into “body size factor.”\n",
    "\n",
    "* **Kernel PCA:**\n",
    "\n",
    "  * Like PCA but can handle **curved/complex relationships**.\n",
    "\n",
    "* **t-SNE:**\n",
    "\n",
    "  * Great for **visualizing** high-dimensional data in 2D or 3D.\n",
    "  * Keeps similar items close together visually.\n",
    "  * **Example:** Plot thousands of handwritten digits in 2D so clusters of 0s, 1s, 2s appear clearly.\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Summary in one line:**\n",
    "\n",
    "* **Clustering →** group similar things\n",
    "* **Dimensionality reduction →** simplify data while keeping patterns\n",
    "* **DBSCAN →** find dense groups and outliers\n",
    "* **PCA/t-SNE →** make data smaller or visualizable"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
