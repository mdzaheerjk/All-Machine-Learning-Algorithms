{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56e236d9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## DT used For Both Classification & Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c74522",
   "metadata": {},
   "source": [
    "**decision tree**\n",
    "\n",
    "* A **supervised learning model** (it learns from labeled training data).\n",
    "* It works by **splitting the dataset** into smaller and smaller groups based on feature values.\n",
    "* At each split (called a **node**), the algorithm chooses the feature and condition (like “Age > 30?”) that best separates the data.\n",
    "* This process continues until the data is pure enough (mostly one label) or until stopping rules are reached (like max depth).\n",
    "* The final points (called **leaves**) hold the prediction:\n",
    "\n",
    "  * For **classification** → a class label (e.g., “spam” or “not spam”).\n",
    "  * For **regression** → a numeric value (e.g., predicted house price).\n",
    "\n",
    "📌 Example in ML:\n",
    "\n",
    "* Suppose you train a decision tree to predict if a student passes an exam.\n",
    "* Features: hours studied, attendance, assignments completed.\n",
    "* The tree might learn rules like:\n",
    "\n",
    "  * If hours studied > 5 → predict **Pass**.\n",
    "  * Else if attendance < 50% → predict **Fail**.\n",
    "  * Else → predict **Pass**.\n",
    "\n",
    "👉 In short: **A decision tree is a model that learns a set of “if-then” rules from data, and uses those rules to predict outcomes.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9b1c6d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## pure and Impure splits\n",
    "\n",
    "### Imagine this:\n",
    "\n",
    "You’re sorting fruits into boxes: 🍎 apples and 🍌 bananas.\n",
    "\n",
    "* **Pure split** → A box has only apples, or only bananas. (Everything inside is the same — nice and clean!)\n",
    "* **Impure split** → A box has a mix of apples and bananas. (Not clear, messy.)\n",
    "\n",
    "### In decision trees:\n",
    "\n",
    "* **Pure split** = All data in that group belongs to one outcome (e.g., all “Yes” or all “No”).\n",
    "* **Impure split** = The group still has a mix of outcomes.\n",
    "\n",
    "👉 The goal of the tree is to keep splitting until the boxes (groups) are as **pure** as possible — meaning each group mostly has one clear answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffe8632",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Tiny Example\n",
    "\n",
    "### Example: Students passing or failing an exam\n",
    "\n",
    "We have 6 students:\n",
    "\n",
    "| Hours studied | Result |\n",
    "| ------------- | ------ |\n",
    "| 8             | Pass   |\n",
    "| 7             | Pass   |\n",
    "| 6             | Pass   |\n",
    "| 2             | Fail   |\n",
    "| 1             | Fail   |\n",
    "| 0             | Fail   |\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Look at all students together\n",
    "\n",
    "* Mixed results: some **Pass**, some **Fail**.\n",
    "* This group is **impure** (not clean).\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Try a split → “Did the student study more than 5 hours?”\n",
    "\n",
    "* **Group 1 (Yes, >5 hrs):** Pass, Pass, Pass → All are **Pass** ✅ (Pure)\n",
    "* **Group 2 (No, ≤5 hrs):** Fail, Fail, Fail → All are **Fail** ✅ (Pure)\n",
    "\n",
    "Now both groups are **pure** because each box only has one result.\n",
    "\n",
    "---\n",
    "\n",
    "👉 That’s the idea:\n",
    "\n",
    "* **Impure split** = a box has both Pass and Fail.\n",
    "* **Pure split** = a box has only Pass or only Fail.\n",
    "\n",
    "The decision tree’s job is to keep asking smart questions until the boxes are as pure as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4512964b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# How to know pure or impure splits\n",
    "\n",
    "## **1. Entropy** – “How mixed up the labels are”\n",
    "\n",
    "* **Measures uncertainty** in a group.\n",
    "* **Pure group** → all examples have the same label → entropy = **0**.\n",
    "* **Impure group** → labels are mixed → entropy is **higher**.\n",
    "* **Range:**\n",
    "\n",
    "  * For **binary classification** (2 classes): 0 to 1\n",
    "\n",
    "    * 0 → pure (all same class)\n",
    "    * 1 → perfectly mixed (50%-50%)\n",
    "  * For **more than 2 classes**, the max value increases slightly (depends on number of classes).\n",
    "\n",
    "**Example (binary Pass/Fail):**\n",
    "\n",
    "* All Pass → entropy = 0 ✅\n",
    "* 2 Pass, 2 Fail → entropy = 1 ❌ (most impure)\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Gini Index / Gini Impurity** – “Chance of picking the wrong label”\n",
    "\n",
    "* **Measures impurity** of a group.\n",
    "* **Pure group** → all examples have the same label → Gini = **0**.\n",
    "* **Impure group** → labels are mixed → Gini is **higher**.\n",
    "* **Range:**\n",
    "\n",
    "  * Binary classification → 0 to 0.5\n",
    "\n",
    "    * 0 → pure\n",
    "    * 0.5 → perfectly mixed (50%-50%)\n",
    "  * For multiple classes → max = 1 − 1/n\\_classes\n",
    "\n",
    "**Example (binary Pass/Fail):**\n",
    "\n",
    "* All Pass → Gini = 0 ✅\n",
    "* 2 Pass, 2 Fail → Gini = 0.5 ❌ (most impure)\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Comparing Entropy & Gini**\n",
    "\n",
    "| Concept | Pure | Impure | Max value (binary) |\n",
    "| ------- | ---- | ------ | ------------------ |\n",
    "| Entropy | 0    | High   | 1                  |\n",
    "| Gini    | 0    | High   | 0.5                |\n",
    "\n",
    "* Both **tell the tree how “mixed” a group is**.\n",
    "* Decision tree chooses splits that **reduce impurity** → makes groups **purer**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b80b68",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Information Gain\n",
    "## What feature you need to select to start constructing DT\n",
    "\n",
    "### Choosing a Feature to Split in Decision Tree\n",
    "\n",
    "* At each step, the tree wants to **split the data into purer groups**.\n",
    "* **Information Gain (IG)** tells us **how much a split reduces messiness** (impurity).\n",
    "* The tree looks at all features and picks the one with the **highest IG** → creates the cleanest child nodes.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "* Predicting Pass/Fail:\n",
    "\n",
    "  * Feature “Hours Studied” → splits into all Pass / all Fail → **high IG** ✅\n",
    "  * Feature “Attendance” → some mixed groups → **lower IG** ❌\n",
    "* Tree chooses **Hours Studied** to split first.\n",
    "\n",
    "**Key idea:** **Pick the feature that makes the groups as pure as possible.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d48a262",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Post pruning & Pre pruning \n",
    "\n",
    "### **1. Pre-Pruning (Stop Early)**\n",
    "\n",
    "* Done **while building the tree**.\n",
    "* The tree **stops growing** if a split won’t make a big difference.\n",
    "* Example rules:\n",
    "\n",
    "  * Don’t split if the group is very small\n",
    "  * Don’t split if the tree is already too deep\n",
    "* Purpose: **keep the tree simple** and avoid memorizing noise in the training data.\n",
    "\n",
    "**Think:** “Stop asking more questions once you’re confident enough.”\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Post-Pruning (Cut Back Later)**\n",
    "\n",
    "* Done **after the tree is fully built**.\n",
    "* The tree is grown completely first, then **unnecessary branches are removed**.\n",
    "* Purpose: remove parts that are too specific to training data and **make the tree simpler**.\n",
    "\n",
    "**Think:** “Ask all questions first, then remove the ones that don’t really help.”\n",
    "\n",
    "---\n",
    "* **Pre-pruning:** Stop early to avoid overfitting\n",
    "* **Post-pruning:** Grow fully, then cut back to improve generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71968a8a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Varience Reduction in DTR\n",
    "\n",
    "### **1. Variance of a group**\n",
    "\n",
    "* Variance measures how **spread out the numbers** are in a group.\n",
    "* **Range:**\n",
    "\n",
    "  * Minimum = 0 → all numbers are the same (perfectly “pure”)\n",
    "  * Maximum = depends on the data → more spread = higher variance\n",
    "\n",
    "**Example:**\n",
    "\n",
    "* Group: 500, 480 → variance small (\\~200) ✅\n",
    "* Group: 500, 250 → variance large (\\~15,625) ❌\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Variance Reduction**\n",
    "\n",
    "* Variance reduction = variance before split − weighted variance after split\n",
    "* **Range:**\n",
    "\n",
    "  * Minimum = 0 → split doesn’t reduce variance (no improvement)\n",
    "  * Maximum = variance before split → best possible split (child nodes perfectly uniform)\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Key Idea:**\n",
    "\n",
    "* Variance = how messy the numbers are\n",
    "* Variance reduction = how much cleaner a split makes the numbers\n",
    "* Goal: pick the feature that **maximizes variance reduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d579e1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### in simple words it is like information gain in DTC to select which feature to start split but variecne Reduction in DTR"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
