{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56e236d9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## DT used For Both Classification & Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c74522",
   "metadata": {},
   "source": [
    "**decision tree**\n",
    "\n",
    "* A **supervised learning model** (it learns from labeled training data).\n",
    "* It works by **splitting the dataset** into smaller and smaller groups based on feature values.\n",
    "* At each split (called a **node**), the algorithm chooses the feature and condition (like â€œAge > 30?â€) that best separates the data.\n",
    "* This process continues until the data is pure enough (mostly one label) or until stopping rules are reached (like max depth).\n",
    "* The final points (called **leaves**) hold the prediction:\n",
    "\n",
    "  * For **classification** â†’ a class label (e.g., â€œspamâ€ or â€œnot spamâ€).\n",
    "  * For **regression** â†’ a numeric value (e.g., predicted house price).\n",
    "\n",
    "ğŸ“Œ Example in ML:\n",
    "\n",
    "* Suppose you train a decision tree to predict if a student passes an exam.\n",
    "* Features: hours studied, attendance, assignments completed.\n",
    "* The tree might learn rules like:\n",
    "\n",
    "  * If hours studied > 5 â†’ predict **Pass**.\n",
    "  * Else if attendance < 50% â†’ predict **Fail**.\n",
    "  * Else â†’ predict **Pass**.\n",
    "\n",
    "ğŸ‘‰ In short: **A decision tree is a model that learns a set of â€œif-thenâ€ rules from data, and uses those rules to predict outcomes.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9b1c6d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## pure and Impure splits\n",
    "\n",
    "### Imagine this:\n",
    "\n",
    "Youâ€™re sorting fruits into boxes: ğŸ apples and ğŸŒ bananas.\n",
    "\n",
    "* **Pure split** â†’ A box has only apples, or only bananas. (Everything inside is the same â€” nice and clean!)\n",
    "* **Impure split** â†’ A box has a mix of apples and bananas. (Not clear, messy.)\n",
    "\n",
    "### In decision trees:\n",
    "\n",
    "* **Pure split** = All data in that group belongs to one outcome (e.g., all â€œYesâ€ or all â€œNoâ€).\n",
    "* **Impure split** = The group still has a mix of outcomes.\n",
    "\n",
    "ğŸ‘‰ The goal of the tree is to keep splitting until the boxes (groups) are as **pure** as possible â€” meaning each group mostly has one clear answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffe8632",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Tiny Example\n",
    "\n",
    "### Example: Students passing or failing an exam\n",
    "\n",
    "We have 6 students:\n",
    "\n",
    "| Hours studied | Result |\n",
    "| ------------- | ------ |\n",
    "| 8             | Pass   |\n",
    "| 7             | Pass   |\n",
    "| 6             | Pass   |\n",
    "| 2             | Fail   |\n",
    "| 1             | Fail   |\n",
    "| 0             | Fail   |\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Look at all students together\n",
    "\n",
    "* Mixed results: some **Pass**, some **Fail**.\n",
    "* This group is **impure** (not clean).\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Try a split â†’ â€œDid the student study more than 5 hours?â€\n",
    "\n",
    "* **Group 1 (Yes, >5 hrs):** Pass, Pass, Pass â†’ All are **Pass** âœ… (Pure)\n",
    "* **Group 2 (No, â‰¤5 hrs):** Fail, Fail, Fail â†’ All are **Fail** âœ… (Pure)\n",
    "\n",
    "Now both groups are **pure** because each box only has one result.\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ‘‰ Thatâ€™s the idea:\n",
    "\n",
    "* **Impure split** = a box has both Pass and Fail.\n",
    "* **Pure split** = a box has only Pass or only Fail.\n",
    "\n",
    "The decision treeâ€™s job is to keep asking smart questions until the boxes are as pure as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4512964b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# How to know pure or impure splits\n",
    "\n",
    "## **1. Entropy** â€“ â€œHow mixed up the labels areâ€\n",
    "\n",
    "* **Measures uncertainty** in a group.\n",
    "* **Pure group** â†’ all examples have the same label â†’ entropy = **0**.\n",
    "* **Impure group** â†’ labels are mixed â†’ entropy is **higher**.\n",
    "* **Range:**\n",
    "\n",
    "  * For **binary classification** (2 classes): 0 to 1\n",
    "\n",
    "    * 0 â†’ pure (all same class)\n",
    "    * 1 â†’ perfectly mixed (50%-50%)\n",
    "  * For **more than 2 classes**, the max value increases slightly (depends on number of classes).\n",
    "\n",
    "**Example (binary Pass/Fail):**\n",
    "\n",
    "* All Pass â†’ entropy = 0 âœ…\n",
    "* 2 Pass, 2 Fail â†’ entropy = 1 âŒ (most impure)\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Gini Index / Gini Impurity** â€“ â€œChance of picking the wrong labelâ€\n",
    "\n",
    "* **Measures impurity** of a group.\n",
    "* **Pure group** â†’ all examples have the same label â†’ Gini = **0**.\n",
    "* **Impure group** â†’ labels are mixed â†’ Gini is **higher**.\n",
    "* **Range:**\n",
    "\n",
    "  * Binary classification â†’ 0 to 0.5\n",
    "\n",
    "    * 0 â†’ pure\n",
    "    * 0.5 â†’ perfectly mixed (50%-50%)\n",
    "  * For multiple classes â†’ max = 1 âˆ’ 1/n\\_classes\n",
    "\n",
    "**Example (binary Pass/Fail):**\n",
    "\n",
    "* All Pass â†’ Gini = 0 âœ…\n",
    "* 2 Pass, 2 Fail â†’ Gini = 0.5 âŒ (most impure)\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Comparing Entropy & Gini**\n",
    "\n",
    "| Concept | Pure | Impure | Max value (binary) |\n",
    "| ------- | ---- | ------ | ------------------ |\n",
    "| Entropy | 0    | High   | 1                  |\n",
    "| Gini    | 0    | High   | 0.5                |\n",
    "\n",
    "* Both **tell the tree how â€œmixedâ€ a group is**.\n",
    "* Decision tree chooses splits that **reduce impurity** â†’ makes groups **purer**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b80b68",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Information Gain\n",
    "## What feature you need to select to start constructing DT\n",
    "\n",
    "### Choosing a Feature to Split in Decision Tree\n",
    "\n",
    "* At each step, the tree wants to **split the data into purer groups**.\n",
    "* **Information Gain (IG)** tells us **how much a split reduces messiness** (impurity).\n",
    "* The tree looks at all features and picks the one with the **highest IG** â†’ creates the cleanest child nodes.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "* Predicting Pass/Fail:\n",
    "\n",
    "  * Feature â€œHours Studiedâ€ â†’ splits into all Pass / all Fail â†’ **high IG** âœ…\n",
    "  * Feature â€œAttendanceâ€ â†’ some mixed groups â†’ **lower IG** âŒ\n",
    "* Tree chooses **Hours Studied** to split first.\n",
    "\n",
    "**Key idea:** **Pick the feature that makes the groups as pure as possible.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d48a262",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Post pruning & Pre pruning \n",
    "\n",
    "### **1. Pre-Pruning (Stop Early)**\n",
    "\n",
    "* Done **while building the tree**.\n",
    "* The tree **stops growing** if a split wonâ€™t make a big difference.\n",
    "* Example rules:\n",
    "\n",
    "  * Donâ€™t split if the group is very small\n",
    "  * Donâ€™t split if the tree is already too deep\n",
    "* Purpose: **keep the tree simple** and avoid memorizing noise in the training data.\n",
    "\n",
    "**Think:** â€œStop asking more questions once youâ€™re confident enough.â€\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Post-Pruning (Cut Back Later)**\n",
    "\n",
    "* Done **after the tree is fully built**.\n",
    "* The tree is grown completely first, then **unnecessary branches are removed**.\n",
    "* Purpose: remove parts that are too specific to training data and **make the tree simpler**.\n",
    "\n",
    "**Think:** â€œAsk all questions first, then remove the ones that donâ€™t really help.â€\n",
    "\n",
    "---\n",
    "* **Pre-pruning:** Stop early to avoid overfitting\n",
    "* **Post-pruning:** Grow fully, then cut back to improve generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71968a8a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Varience Reduction in DTR\n",
    "\n",
    "### **1. Variance of a group**\n",
    "\n",
    "* Variance measures how **spread out the numbers** are in a group.\n",
    "* **Range:**\n",
    "\n",
    "  * Minimum = 0 â†’ all numbers are the same (perfectly â€œpureâ€)\n",
    "  * Maximum = depends on the data â†’ more spread = higher variance\n",
    "\n",
    "**Example:**\n",
    "\n",
    "* Group: 500, 480 â†’ variance small (\\~200) âœ…\n",
    "* Group: 500, 250 â†’ variance large (\\~15,625) âŒ\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Variance Reduction**\n",
    "\n",
    "* Variance reduction = variance before split âˆ’ weighted variance after split\n",
    "* **Range:**\n",
    "\n",
    "  * Minimum = 0 â†’ split doesnâ€™t reduce variance (no improvement)\n",
    "  * Maximum = variance before split â†’ best possible split (child nodes perfectly uniform)\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **Key Idea:**\n",
    "\n",
    "* Variance = how messy the numbers are\n",
    "* Variance reduction = how much cleaner a split makes the numbers\n",
    "* Goal: pick the feature that **maximizes variance reduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d579e1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### in simple words it is like information gain in DTC to select which feature to start split but variecne Reduction in DTR"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
