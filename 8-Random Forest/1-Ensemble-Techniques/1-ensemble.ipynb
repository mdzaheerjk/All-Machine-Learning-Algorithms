{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e30075b",
   "metadata": {},
   "source": [
    "# ğŸŒŸ Ensembling in ML\n",
    "\n",
    "Ensembling means: instead of trusting **one model**, we train **many models** and then combine them to make a stronger final prediction.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸŸ¦ Bagging (Bootstrap Aggregating)\n",
    "\n",
    "* **How it works:**\n",
    "\n",
    "  1. Take your dataset.\n",
    "  2. Randomly create different subsets (by sampling with replacement).\n",
    "  3. Train a separate model (often the same type, like decision trees) on each subset.\n",
    "  4. Combine their predictions:\n",
    "\n",
    "     * For classification â†’ majority vote.\n",
    "     * For regression â†’ average.\n",
    "\n",
    "* **ML effect:**\n",
    "\n",
    "  * Reduces **variance** (models wonâ€™t overfit as much).\n",
    "  * Works well with **unstable models** like decision trees.\n",
    "\n",
    "* **Example:** Random Forest = many decision trees trained with bagging.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸŸ¨ Boosting\n",
    "\n",
    "* **How it works:**\n",
    "\n",
    "  1. Train the first model on the dataset.\n",
    "  2. Look at where it makes mistakes (misclassified points or large errors).\n",
    "  3. Train the next model, giving **more weight** to those mistakes.\n",
    "  4. Repeat this process so each new model focuses on fixing previous errors.\n",
    "  5. Combine all modelsâ€™ predictions (weighted sum).\n",
    "\n",
    "* **ML effect:**\n",
    "\n",
    "  * Reduces **bias** (turns weak learners into a strong learner).\n",
    "  * Can capture complex patterns, but might risk overfitting if not controlled.\n",
    "\n",
    "* **Examples:** AdaBoost, Gradient Boosting, XGBoost, LightGBM.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§  Intuition\n",
    "\n",
    "* **Bagging** = â€œTrain models **in parallel** on random subsets and combine them.â€\n",
    "* **Boosting** = â€œTrain models **in sequence**, each fixing the mistakes of the previous one.â€\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ‘‰ In ML:\n",
    "\n",
    "* Bagging = lowers **variance**.\n",
    "* Boosting = lowers **bias**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030e5631",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### ğŸ”¹ What is **Bias** in ML?\n",
    "\n",
    "* Bias = **error from wrong assumptions**.\n",
    "* If a model is too simple, it might **miss important patterns**.\n",
    "* Example: Using a straight line to fit data that actually curves.\n",
    "* This is called **underfitting**.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¹ What is **Variance** in ML?\n",
    "\n",
    "* Variance = **error from being too sensitive to the training data**.\n",
    "* If a model is too complex, it might **memorize noise** in the data.\n",
    "* Example: A decision tree that splits too much and fits perfectly to training data but fails on new data.\n",
    "* This is called **overfitting**.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸŸ¦ Bagging â†’ reduces Variance\n",
    "\n",
    "* Bagging trains models on different **random subsets** and then averages their predictions.\n",
    "* Averaging cancels out the â€œnoiseâ€ that an overfitted model might pick up.\n",
    "* âœ… So bagging makes unstable models (like deep decision trees) **more stable and less overfitted**.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸŸ¨ Boosting â†’ reduces Bias\n",
    "\n",
    "* Boosting builds models **sequentially**, each focusing on mistakes of the previous.\n",
    "* This forces the final model to learn patterns it missed earlier.\n",
    "* âœ… So boosting makes weak/simple models (like shallow decision trees) **more accurate and less underfitted**.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§  Easy analogy (with ML twist)\n",
    "\n",
    "* **Bagging** = You ask many decision trees separately, then take their average. Any one tree may overfit, but averaging smooths out the noise â†’ less variance.\n",
    "* **Boosting** = You train one weak tree at a time, each trying to fix the errors of the last one. The team of weak learners becomes strong â†’ less bias.\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ‘‰ In short:\n",
    "\n",
    "* Bagging fixes the problem of **models being too wiggly (high variance)**.\n",
    "* Boosting fixes the problem of **models being too simple (high bias)**."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
