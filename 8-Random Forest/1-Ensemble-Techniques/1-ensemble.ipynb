{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e30075b",
   "metadata": {},
   "source": [
    "# 🌟 Ensembling in ML\n",
    "\n",
    "Ensembling means: instead of trusting **one model**, we train **many models** and then combine them to make a stronger final prediction.\n",
    "\n",
    "---\n",
    "\n",
    "### 🟦 Bagging (Bootstrap Aggregating)\n",
    "\n",
    "* **How it works:**\n",
    "\n",
    "  1. Take your dataset.\n",
    "  2. Randomly create different subsets (by sampling with replacement).\n",
    "  3. Train a separate model (often the same type, like decision trees) on each subset.\n",
    "  4. Combine their predictions:\n",
    "\n",
    "     * For classification → majority vote.\n",
    "     * For regression → average.\n",
    "\n",
    "* **ML effect:**\n",
    "\n",
    "  * Reduces **variance** (models won’t overfit as much).\n",
    "  * Works well with **unstable models** like decision trees.\n",
    "\n",
    "* **Example:** Random Forest = many decision trees trained with bagging.\n",
    "\n",
    "---\n",
    "\n",
    "### 🟨 Boosting\n",
    "\n",
    "* **How it works:**\n",
    "\n",
    "  1. Train the first model on the dataset.\n",
    "  2. Look at where it makes mistakes (misclassified points or large errors).\n",
    "  3. Train the next model, giving **more weight** to those mistakes.\n",
    "  4. Repeat this process so each new model focuses on fixing previous errors.\n",
    "  5. Combine all models’ predictions (weighted sum).\n",
    "\n",
    "* **ML effect:**\n",
    "\n",
    "  * Reduces **bias** (turns weak learners into a strong learner).\n",
    "  * Can capture complex patterns, but might risk overfitting if not controlled.\n",
    "\n",
    "* **Examples:** AdaBoost, Gradient Boosting, XGBoost, LightGBM.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Intuition\n",
    "\n",
    "* **Bagging** = “Train models **in parallel** on random subsets and combine them.”\n",
    "* **Boosting** = “Train models **in sequence**, each fixing the mistakes of the previous one.”\n",
    "\n",
    "---\n",
    "\n",
    "👉 In ML:\n",
    "\n",
    "* Bagging = lowers **variance**.\n",
    "* Boosting = lowers **bias**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030e5631",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 🔹 What is **Bias** in ML?\n",
    "\n",
    "* Bias = **error from wrong assumptions**.\n",
    "* If a model is too simple, it might **miss important patterns**.\n",
    "* Example: Using a straight line to fit data that actually curves.\n",
    "* This is called **underfitting**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 What is **Variance** in ML?\n",
    "\n",
    "* Variance = **error from being too sensitive to the training data**.\n",
    "* If a model is too complex, it might **memorize noise** in the data.\n",
    "* Example: A decision tree that splits too much and fits perfectly to training data but fails on new data.\n",
    "* This is called **overfitting**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🟦 Bagging → reduces Variance\n",
    "\n",
    "* Bagging trains models on different **random subsets** and then averages their predictions.\n",
    "* Averaging cancels out the “noise” that an overfitted model might pick up.\n",
    "* ✅ So bagging makes unstable models (like deep decision trees) **more stable and less overfitted**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🟨 Boosting → reduces Bias\n",
    "\n",
    "* Boosting builds models **sequentially**, each focusing on mistakes of the previous.\n",
    "* This forces the final model to learn patterns it missed earlier.\n",
    "* ✅ So boosting makes weak/simple models (like shallow decision trees) **more accurate and less underfitted**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Easy analogy (with ML twist)\n",
    "\n",
    "* **Bagging** = You ask many decision trees separately, then take their average. Any one tree may overfit, but averaging smooths out the noise → less variance.\n",
    "* **Boosting** = You train one weak tree at a time, each trying to fix the errors of the last one. The team of weak learners becomes strong → less bias.\n",
    "\n",
    "---\n",
    "\n",
    "👉 In short:\n",
    "\n",
    "* Bagging fixes the problem of **models being too wiggly (high variance)**.\n",
    "* Boosting fixes the problem of **models being too simple (high bias)**."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
