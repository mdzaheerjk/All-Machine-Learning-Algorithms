{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17028110",
   "metadata": {},
   "source": [
    "### 🟨 Boosting  \n",
    "\n",
    "**Idea:** Instead of training all models separately (like in Bagging), Boosting trains models **one after another**, where each new model tries to **fix the mistakes** made by the previous ones.  \n",
    "\n",
    "---\n",
    "\n",
    "### 📘 Step-by-step with a small example  \n",
    "\n",
    "1. **Dataset**  \n",
    "Say we want to predict whether a student will pass (Yes/No) based on study hours and attendance.  \n",
    "\n",
    "2. **First model (weak learner)**  \n",
    "- Train a simple decision tree.  \n",
    "- It correctly predicts many students but misclassifies some (e.g., says \"Fail\" when they should \"Pass\").  \n",
    "\n",
    "3. **Focus on mistakes**  \n",
    "- The algorithm gives more weight to the students that were predicted incorrectly.  \n",
    "- These “hard” cases become more important for the next model.  \n",
    "\n",
    "4. **Second model**  \n",
    "- Train another tree, but this time it focuses more on the students the first model got wrong.  \n",
    "\n",
    "5. **Repeat**  \n",
    "- Add more trees, each one improving where the last ones failed.  \n",
    "\n",
    "6. **Final prediction**  \n",
    "- Combine all trees’ outputs with weights (better trees get more say).  \n",
    "- Result: a stronger model that captures patterns much better than a single tree.  \n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Small Example (classification)  \n",
    "Predict if a student passes:  \n",
    "\n",
    "- Tree 1: Predicts 8 correct, 2 wrong.  \n",
    "- Tree 2: Focuses on those 2 mistakes → fixes 1, still misses 1.  \n",
    "- Tree 3: Focuses on the last mistake → fixes it.  \n",
    "**Final Boosting model = combines all trees → 10 correct.**  \n",
    "\n",
    "---\n",
    "\n",
    "### 🔑 In ML terms  \n",
    "- Boosting = **sequential training of models**, each correcting errors of the previous.  \n",
    "- Final prediction = **weighted sum of all models**.  \n",
    "- Benefit = reduces **bias** (turns weak learners into a strong learner).  \n",
    "\n",
    "---\n",
    "\n",
    "👉 So:  \n",
    "- **Bagging = parallel teamwork (reduce variance).**  \n",
    "- **Boosting = sequential teamwork (reduce bias).**  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
