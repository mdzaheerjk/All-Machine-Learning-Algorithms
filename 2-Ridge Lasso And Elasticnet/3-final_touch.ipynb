{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edd51733",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "## Ridge regression\n",
    "Linear Regression + penalty on large coefficients (L2 norm) → gives a more stable, less overfit model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599e884f",
   "metadata": {},
   "source": [
    "### ✅ Use Ridge Regression when:\n",
    "\n",
    "1. **You have multicollinearity** (features are correlated).\n",
    "2. **Your model is overfitting** (good training accuracy, poor test accuracy).\n",
    "3. **You want to keep all features** but just reduce their influence (shrink, not remove).\n",
    "4. **You expect coefficients to be small but not exactly zero** (like in polynomial regression or high-dimensional data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4f7448",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# small example\n",
    "\n",
    "### 📊 Data\n",
    "\n",
    "Suppose we have just 3 points:\n",
    "\n",
    "| x | y |\n",
    "| - | - |\n",
    "| 1 | 2 |\n",
    "| 2 | 4 |\n",
    "| 3 | 6 |\n",
    "\n",
    "This is a **perfect line**: $y = 2x$.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Plain Linear Regression\n",
    "\n",
    "* Model can fit exactly:\n",
    "\n",
    "  $$\n",
    "  y = 2x\n",
    "  $$\n",
    "* **Training cost = 0** (no error).\n",
    "* But if we add a noisy point later (say $x=4, y=9$ instead of 8), the model will struggle because it only memorized the perfect line.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 With Ridge Regression\n",
    "\n",
    "* Ridge says: “Fit the line, but don’t let coefficients get too extreme.”\n",
    "* So instead of exactly $m = 2, b = 0$, Ridge might give something like:\n",
    "\n",
    "  $$\n",
    "  y = 1.9x + 0.1\n",
    "  $$\n",
    "* **Training cost > 0** (a little error).\n",
    "* But on noisy/unseen data, it performs **better** because weights are smaller and more stable.\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Takeaway:**\n",
    "\n",
    "* **Plain regression**: memorizes perfectly → cost = 0, risk of overfit.\n",
    "* **Ridge regression**: allows tiny training error but improves **generalization**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3842bb3a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Lasso regression \n",
    "Linear Regression + L1 penalty → makes the model simpler by shrinking some feature weights to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a57ef3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "✅ Use Lasso Regression when:\n",
    "\n",
    "1. You have **many features** but only a few are truly important.\n",
    "2. You want the model to **automatically remove irrelevant features** (by setting their weights to zero).\n",
    "3. You care about a **simpler, more interpretable model**.\n",
    "\n",
    "## In short:\n",
    "**Use Lasso when you want prediction + feature selection at the same time.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36286806",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 📊 Example Data\n",
    "\n",
    "Suppose we want to predict a score (`y`) using 3 features:\n",
    "\n",
    "| x1 | x2 | x3 | y  |\n",
    "| -- | -- | -- | -- |\n",
    "| 1  | 2  | 0  | 10 |\n",
    "| 2  | 4  | 0  | 20 |\n",
    "| 3  | 6  | 0  | 30 |\n",
    "| 4  | 8  | 0  | 40 |\n",
    "\n",
    "Here:\n",
    "\n",
    "* `x1` and `x2` are useful (they explain y).\n",
    "* `x3` is **useless** (always 0).\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Plain Linear Regression\n",
    "\n",
    "It will try to give **some weight** to every feature, even the useless one:\n",
    "\n",
    "* $y \\approx 0.1x1 + 4.9x2 + 0.05x3$\n",
    "* Notice `x3` got a tiny weight (not exactly zero).\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Lasso Regression\n",
    "\n",
    "Because of the L1 penalty, Lasso will **shrink useless features to zero**:\n",
    "\n",
    "* $y \\approx 0.2x1 + 4.8x2 + 0x3$\n",
    "* 👉 `x3` dropped out completely.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Takeaway\n",
    "\n",
    "* **Plain regression**: keeps all features, even useless ones.\n",
    "* **Lasso regression**: keeps only important features, sets useless ones to **0**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a30827a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 🔹 What is Elastic Net Regression?\n",
    "\n",
    "Elastic Net = **Linear Regression with both Ridge (L2) and Lasso (L1) penalties**.\n",
    "\n",
    "$$\n",
    "J(w) = \\text{MSE} + \\lambda_1 \\sum |w_j| + \\lambda_2 \\sum w_j^2\n",
    "$$\n",
    "\n",
    "* **Lasso part (L1):** can set useless features to 0 (feature selection).\n",
    "* **Ridge part (L2):** shrinks large weights to keep them stable when features are correlated.\n",
    "\n",
    "👉 It combines the strengths of both.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 When to Use Elastic Net\n",
    "\n",
    "✅ Use **Elastic Net** when:\n",
    "\n",
    "1. You have **many features**, some irrelevant, some correlated.\n",
    "2. You want **feature selection** (like Lasso) but also **stability with correlated features** (like Ridge).\n",
    "3. You’re not sure whether Ridge or Lasso alone is best → Elastic Net is a safer middle ground.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 Simple Example\n",
    "\n",
    "### Data\n",
    "\n",
    "| x1 | x2 | x3 | y  |\n",
    "| -- | -- | -- | -- |\n",
    "| 1  | 2  | 0  | 10 |\n",
    "| 2  | 4  | 0  | 20 |\n",
    "| 3  | 6  | 0  | 30 |\n",
    "| 4  | 8  | 0  | 40 |\n",
    "\n",
    "* `x1` and `x2` are correlated (x2 = 2 × x1).\n",
    "* `x3` is useless (always 0).\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Plain Linear Regression\n",
    "\n",
    "* Might give unstable weights, e.g.:\n",
    "  $y = 0 \\cdot x1 + 5 \\cdot x2 + 0.1 \\cdot x3$\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Lasso\n",
    "\n",
    "* Drops `x3` completely, but may randomly choose between `x1` or `x2` (since they’re correlated).\n",
    "  $y = 0 \\cdot x1 + 5 \\cdot x2 + 0 \\cdot x3$\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Ridge\n",
    "\n",
    "* Keeps both `x1` and `x2`, shrinks them evenly, but doesn’t drop `x3`.\n",
    "  $y = 2.4 \\cdot x1 + 2.4 \\cdot x2 + 0.05 \\cdot x3$\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Elastic Net\n",
    "\n",
    "* Drops the useless feature (`x3` like Lasso).\n",
    "* Shares weights more fairly between correlated features (`x1`, `x2` like Ridge).\n",
    "  $y = 2.3 \\cdot x1 + 2.3 \\cdot x2 + 0 \\cdot x3$\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Takeaway:**\n",
    "\n",
    "* Lasso → feature selection but unstable with correlated features.\n",
    "* Ridge → stable with correlated features but keeps everything.\n",
    "* **Elastic Net → best of both: drops useless features, keeps stability with correlated ones.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87055fd7",
   "metadata": {},
   "source": [
    "# Ridge() Ridgecv() Lasso() Lassocv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0c3d4b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 1. **Ridge()**\n",
    "\n",
    "* Think of it as **regular linear regression** but with a tiny rule that stops the model from giving too much importance to any single feature.\n",
    "* You **set a strength (`alpha`)** yourself.\n",
    "* Helps prevent the model from **memorizing the data** (overfitting).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Lasso()**\n",
    "\n",
    "* Like Ridge, but **stronger**: it can actually **ignore unimportant features** by setting their weight to zero.\n",
    "* You **choose the strength (`alpha`)**.\n",
    "* Useful if you want the model to focus on the **most important features only**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **RidgeCV()**\n",
    "\n",
    "* Same as Ridge, but smarter: it **automatically finds the best alpha** for you.\n",
    "* It does this by testing different alphas using **cross-validation**.\n",
    "* Saves you from guessing the right alpha.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **LassoCV()**\n",
    "\n",
    "* Same as Lasso, but **automatically finds the best alpha** using cross-validation.\n",
    "* Shrinks unimportant features to zero **and** chooses the best strength for the penalty.\n",
    "\n",
    "---\n",
    "\n",
    "✅ **In short:**\n",
    "\n",
    "| Method    | Manual or Auto alpha? | Feature Selection? | What it does                                                      |\n",
    "| --------- | --------------------- | ------------------ | ----------------------------------------------------------------- |\n",
    "| Ridge()   | Manual                | No                 | Shrinks coefficients slightly to prevent overfitting              |\n",
    "| Lasso()   | Manual                | Yes                | Shrinks some coefficients to zero, keeps only important features  |\n",
    "| RidgeCV() | Automatic             | No                 | Shrinks coefficients + chooses best alpha                         |\n",
    "| LassoCV() | Automatic             | Yes                | Shrinks coefficients, drops unimportant ones + chooses best alpha |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
