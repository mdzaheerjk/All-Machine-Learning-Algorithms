{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cdaa013",
   "metadata": {},
   "source": [
    "# 1. **AdaBoost (Adaptive Boosting)**\n",
    "\n",
    "* **Idea:** Each new weak learner gives more attention (higher weights) to the instances that were misclassified by earlier learners.\n",
    "* **Mechanism:**\n",
    "\n",
    "  * Start with equal weights for all samples.\n",
    "  * After each tree, increase weights of misclassified points and decrease weights of correctly classified ones.\n",
    "  * Final prediction = weighted vote of all learners.\n",
    "* **Strengths:** Simple, works well for clean data.\n",
    "* **Weaknesses:** Sensitive to noisy data & outliers, because weights on hard points can become very large.\n",
    "\n",
    "---\n",
    "\n",
    "# 2. **Gradient Boosting**\n",
    "\n",
    "* **Idea:** Instead of adjusting sample weights, it **fits new learners to the residual errors** (the difference between predictions and true values).\n",
    "* **Mechanism:**\n",
    "\n",
    "  * Train the first tree.\n",
    "  * Compute residuals (errors).\n",
    "  * Train the next tree to predict the residuals.\n",
    "  * Repeat and combine them.\n",
    "* **Strengths:** More flexible (uses gradient descent concept, not just weights).\n",
    "* **Weaknesses:** Slower than AdaBoost, prone to overfitting without tuning (requires learning rate, tree depth, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "# 3. **XGBoost (Extreme Gradient Boosting)**\n",
    "\n",
    "* **Idea:** An optimized, regularized version of gradient boosting.\n",
    "* **Key Improvements:**\n",
    "\n",
    "  * Uses **second-order derivatives** (like Newton’s method) for more precise gradient updates.\n",
    "  * Adds **regularization (L1 & L2)** to reduce overfitting.\n",
    "  * Optimized for speed with parallelization and distributed computing.\n",
    "  * Automatically handles missing values.\n",
    "* **Strengths:** Fast, accurate, and widely used in ML competitions.\n",
    "* **Weaknesses:** More complex, can be memory-intensive for large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "✅ **In short:**\n",
    "\n",
    "* **AdaBoost**: reweights misclassified points → focuses on hard examples.\n",
    "* **Gradient Boosting**: fits learners to residual errors → uses gradient descent.\n",
    "* **XGBoost**: enhanced gradient boosting → faster, regularized, more robust."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
