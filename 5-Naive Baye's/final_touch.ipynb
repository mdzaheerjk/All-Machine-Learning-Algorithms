{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12c060db",
   "metadata": {},
   "source": [
    "# Little Math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d539d54",
   "metadata": {},
   "source": [
    "### 1. Bayes‚Äô Theorem\n",
    "\n",
    "Bayes‚Äô theorem tells us how to calculate the probability of something (say $A$) happening when we already know something else (say $B$) happened:\n",
    "\n",
    "$$\n",
    "P(A \\mid B) = \\frac{P(B \\mid A) \\cdot P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "* $P(A \\mid B)$ = probability of $A$ given $B$\n",
    "* $P(B \\mid A)$ = probability of $B$ given $A$\n",
    "* $P(A)$ = overall probability of $A$\n",
    "* $P(B)$ = overall probability of $B$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Example (Bayes in daily life)\n",
    "\n",
    "Suppose:\n",
    "\n",
    "* 1% of emails are spam.\n",
    "* If an email is spam, there‚Äôs a 90% chance it contains the word *‚Äúoffer‚Äù*.\n",
    "* If an email is not spam, there‚Äôs a 20% chance it contains the word *‚Äúoffer‚Äù*.\n",
    "\n",
    "Now, if an email has the word *‚Äúoffer‚Äù*, what‚Äôs the probability it‚Äôs spam?\n",
    "That‚Äôs exactly where Bayes‚Äô theorem helps.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Extending to Many Features\n",
    "\n",
    "In real problems (like text classification), we don‚Äôt look at just one word, but many words (like *offer*, *win*, *money*, etc.).\n",
    "\n",
    "So, if our class is **Spam** or **Not Spam**, and we have features (words) $x_1, x_2, x_3, \\dots, x_n$, Bayes says:\n",
    "\n",
    "$$\n",
    "P(\\text{Class} \\mid x_1, x_2, \\dots, x_n) \n",
    "= \\frac{P(x_1, x_2, \\dots, x_n \\mid \\text{Class}) \\cdot P(\\text{Class})}{P(x_1, x_2, \\dots, x_n)}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 4. The ‚ÄúNaive‚Äù Assumption\n",
    "\n",
    "Here‚Äôs the tricky part:\n",
    "Calculating $P(x_1, x_2, \\dots, x_n \\mid \\text{Class})$ is **very hard** because it needs the joint probability of all features together.\n",
    "\n",
    "So Naive Bayes makes a **simplifying assumption**:\n",
    "üëâ All features are independent given the class.\n",
    "\n",
    "This means:\n",
    "\n",
    "$$\n",
    "P(x_1, x_2, \\dots, x_n \\mid \\text{Class}) \\approx P(x_1 \\mid \\text{Class}) \\cdot P(x_2 \\mid \\text{Class}) \\cdot \\dots \\cdot P(x_n \\mid \\text{Class})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Naive Bayes Formula\n",
    "\n",
    "Now, the formula becomes:\n",
    "\n",
    "$$\n",
    "P(\\text{Class} \\mid x_1, x_2, \\dots, x_n) \\propto P(\\text{Class}) \\cdot \\prod_{i=1}^{n} P(x_i \\mid \\text{Class})\n",
    "$$\n",
    "\n",
    "The denominator $P(x_1, x_2, \\dots, x_n)$ is the same for all classes, so we usually ignore it when comparing.\n",
    "\n",
    "So the rule is:\n",
    "üëâ For each class, multiply its prior probability ($P(\\text{Class})$) by the likelihoods of each feature.\n",
    "üëâ Pick the class with the highest score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0849b218",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# About Algorithm\n",
    "\n",
    "### What is Naive Bayes?\n",
    "\n",
    "* **Naive Bayes** is a **simple algorithm** used to **classify things into categories** (like spam vs not spam, positive review vs negative review, etc.).\n",
    "* It is based on **probability** ‚Äî it tries to find which category is **most likely** for a given piece of data.\n",
    "* The word **‚ÄúNaive‚Äù** comes from the fact that it assumes all features (clues) are independent from each other.\n",
    "* The word **‚ÄúBayes‚Äù** comes from **Bayes‚Äô Theorem**, the math rule it uses.\n",
    "\n",
    "---\n",
    "\n",
    "### How it works (super short steps):\n",
    "\n",
    "1. Look at your data (features/clues).\n",
    "2. For each possible category, calculate a probability score.\n",
    "3. Choose the category with the **highest score**.\n",
    "\n",
    "---\n",
    "\n",
    "üëâ In short:\n",
    "**Naive Bayes is a simple probability-based method that guesses the category of data by multiplying probabilities of clues and picking the most likely category.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed376c53",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Varients of Naive Bayes\n",
    "\n",
    "### 1. **Bernoulli Naive Bayes**\n",
    "\n",
    "* **What it is:** Works with **binary features** (yes/no, 0/1, present/absent).\n",
    "* **When to use:** When data is about **whether a feature exists or not** (not how many times).\n",
    "* **Tiny Example:**\n",
    "\n",
    "  * Email classification:\n",
    "\n",
    "    * Feature = ‚ÄúDoes the email contain the word *offer*?‚Äù\n",
    "    * Answer = Yes (1) or No (0).\n",
    "  * Bernoulli NB looks at the **presence/absence** of words, not counts.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Multinomial Naive Bayes**\n",
    "\n",
    "* **What it is:** Works with **counts of features** (how many times something appears).\n",
    "* **When to use:** For **text data** where word frequency matters.\n",
    "* **Tiny Example:**\n",
    "\n",
    "  * Email classification:\n",
    "\n",
    "    * Word ‚Äúoffer‚Äù appears **3 times**.\n",
    "    * Word ‚Äúwin‚Äù appears **1 time**.\n",
    "  * Multinomial NB uses these **counts** to calculate probabilities.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Gaussian Naive Bayes**\n",
    "\n",
    "* **What it is:** Works with **continuous (numeric) features**, assuming values follow a **bell curve (Gaussian distribution)**.\n",
    "* **When to use:** When features are **real numbers** like height, weight, temperature, exam scores, etc.\n",
    "* **Tiny Example:**\n",
    "\n",
    "  * Predict if a fruit is ‚Äúapple‚Äù or ‚Äúorange‚Äù using **weight** and **diameter** (continuous values).\n",
    "  * Gaussian NB assumes these numbers follow a normal distribution for each class.\n",
    "\n",
    "---\n",
    "\n",
    "### Quick Summary\n",
    "\n",
    "* **Bernoulli:** Features are **yes/no** ‚Üí good for binary text data.\n",
    "* **Multinomial:** Features are **counts** ‚Üí good for word frequency in documents.\n",
    "* **Gaussian:** Features are **continuous numbers** ‚Üí good for measurements.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
