{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e9a5378",
   "metadata": {},
   "source": [
    "# ğŸŒŸ KNN Algorithm â€“ Simple Steps (with ML wording)\n",
    "\n",
    "### Step 1: **Pick K**\n",
    "\n",
    "* K = number of neighbors you will look at.\n",
    "* Example: K = 3 â†’ weâ€™ll check the 3 closest points.\n",
    "\n",
    "ğŸ‘‰ In ML, **K is a hyperparameter** (something we choose before running the model).\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: **Measure Distance**\n",
    "\n",
    "* To know who is â€œclosest,â€ we calculate **distance**.\n",
    "* Most common: **Euclidean distance** (like using a ruler on a graph).\n",
    "\n",
    "ğŸ‘‰ In ML, this is the **distance metric**.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: **Find K Nearest Neighbors**\n",
    "\n",
    "* From all the points in the dataset, select the **K closest ones** to the new point.\n",
    "\n",
    "ğŸ‘‰ In ML, we call this **retrieving neighbors**.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: **Make Prediction**\n",
    "\n",
    "Now comes the difference:\n",
    "\n",
    "#### ğŸ”¹ For **Classification**\n",
    "\n",
    "* Look at the labels of the K neighbors (e.g., Apple ğŸ or Orange ğŸŠ).\n",
    "* The label with the **most votes** becomes the prediction.\n",
    "  ğŸ‘‰ In ML: **majority vote rule**.\n",
    "\n",
    "#### ğŸ”¹ For **Regression**\n",
    "\n",
    "* Look at the values of the K neighbors (e.g., house prices ğŸ’°).\n",
    "* Take the **average (or weighted average)** of those values.\n",
    "  ğŸ‘‰ In ML: **averaging rule**.\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ¯ Putting it Together\n",
    "\n",
    "* **Classification with KNN** â†’ Predict the **category** by majority vote.\n",
    "* **Regression with KNN** â†’ Predict the **number** by averaging values.\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ§  Easy Analogy\n",
    "\n",
    "* Classification = Asking your neighbors: *â€œDo you drink tea or coffee?â€* â†’ You pick the drink most neighbors prefer.\n",
    "* Regression = Asking your neighbors: *â€œHow much do you spend on coffee per week?â€* â†’ You take the average of their answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdc8a19",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Euclidean vs Manhattan Distance\n",
    "\n",
    "\n",
    "## ğŸ”¹ Euclidean Distance (straight-line distance)\n",
    "\n",
    "Think of it like:\n",
    "\n",
    "* Youâ€™re standing at one point in a field.\n",
    "* You want to get to another point.\n",
    "* If you could fly **like a bird**, youâ€™d go in a **straight line**.\n",
    "\n",
    "ğŸ‘‰ That straight line is **Euclidean distance**.\n",
    "Itâ€™s the â€œas-the-crow-fliesâ€ distance.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ Manhattan Distance (grid distance)\n",
    "\n",
    "Now imagine youâ€™re in a city with streets laid out like a grid (like Manhattan in New York ğŸ™ï¸).\n",
    "\n",
    "* Youâ€™re at one corner.\n",
    "* You want to get to another corner.\n",
    "* You **canâ€™t cut across buildings**â€”you must walk along the streets.\n",
    "\n",
    "ğŸ‘‰ The total number of blocks you walk (up + across) is **Manhattan distance**.\n",
    "Itâ€™s the â€œcity blockâ€ distance.\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ¯ Simple Example\n",
    "\n",
    "Suppose youâ€™re at point **(0, 0)** and want to reach **(3, 4)**:\n",
    "\n",
    "* **Euclidean distance** = straight line (like using Pythagorasâ€™ theorem â†’ 5).\n",
    "* **Manhattan distance** = walk 3 steps right + 4 steps up = 7.\n",
    "\n",
    "---\n",
    "\n",
    "âœ… **In short:**\n",
    "\n",
    "* **Euclidean = straight line**\n",
    "* **Manhattan = grid path**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ccf8a2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# KD-Tree & Ball-Tree \n",
    "## For optimizing search in KNN\n",
    "\n",
    "### ğŸŒ³ KD Tree (K-Dimensional Tree)\n",
    "\n",
    "* Itâ€™s a **data structure** that organizes points in space.\n",
    "* It works by **repeatedly splitting the dataset** along one feature at a time (e.g., first split by height, then by weight, etc.).\n",
    "* This creates a tree where each branch represents a smaller region of the data.\n",
    "* During KNN search, you donâ€™t check all points â€” you only search the **relevant branches** of the tree.\n",
    "\n",
    "ğŸ‘‰ In ML terms: **KD Tree speeds up nearest-neighbor search when data has fewer features (low dimensions).**\n",
    "\n",
    "---\n",
    "\n",
    "### âšª Ball Tree\n",
    "\n",
    "* Another **data structure** for organizing points.\n",
    "* Instead of splitting by features, it **groups points into hyperspheres** (think â€œballsâ€ around clusters of points).\n",
    "* Each ball contains points that are close to each other.\n",
    "* During KNN search, you can quickly eliminate balls that are too far away, so you only check the useful ones.\n",
    "\n",
    "ğŸ‘‰ In ML terms: **Ball Tree is more effective than KD Tree for higher-dimensional data.**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”‘ Why are they important in ML?\n",
    "\n",
    "* KNN requires finding **nearest neighbors**, which can be slow if you check every point.\n",
    "* **KD Tree and Ball Tree are efficient search structures** that reduce the number of comparisons.\n",
    "* They make KNN practical for larger datasets.\n",
    "\n",
    "---\n",
    "\n",
    "âœ… **Summary in ML terms (simple)**\n",
    "\n",
    "* **KD Tree** â†’ splits space by features (best for low dimensions).\n",
    "* **Ball Tree** â†’ groups points into balls (better for high dimensions).\n",
    "* Both â†’ **faster nearest-neighbor search** â†’ makes KNN more efficient."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
