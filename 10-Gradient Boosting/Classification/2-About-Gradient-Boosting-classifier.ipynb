{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d59ea08",
   "metadata": {},
   "source": [
    "# **Gradient Boosting classifier using 3 weak learners**\n",
    "\n",
    "We want to classify fruits as **Apple (0)** or **Orange (1)** based on **size**.\n",
    "\n",
    "**Training data:**\n",
    "\n",
    "| Size | Label      |\n",
    "| ---- | ---------- |\n",
    "| 1    | Apple (0)  |\n",
    "| 2    | Apple (0)  |\n",
    "| 3    | Orange (1) |\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: First weak learner (Tree #1)\n",
    "\n",
    "* Predicts the **majority class** ‚Üí Apple (0) for everything.\n",
    "\n",
    "| Size | True Label | Pred (Tree #1) | Error     |\n",
    "| ---- | ---------- | -------------- | --------- |\n",
    "| 1    | 0          | 0              | ‚úÖ correct |\n",
    "| 2    | 0          | 0              | ‚úÖ correct |\n",
    "| 3    | 1          | 0              | ‚ùå mistake |\n",
    "\n",
    "So, only Size=3 is misclassified.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Second weak learner (Tree #2)\n",
    "\n",
    "This tree focuses on the **residual errors** (the misclassified points).\n",
    "\n",
    "* Learns: ‚ÄúIf Size=3 ‚Üí predict Orange (1).‚Äù\n",
    "\n",
    "Now combine Tree #1 and Tree #2 (weighted sum of their votes, then apply sigmoid/softmax):\n",
    "\n",
    "* Size=1 ‚Üí Still Apple ‚úÖ\n",
    "* Size=2 ‚Üí Still Apple ‚úÖ\n",
    "* Size=3 ‚Üí Corrected to Orange ‚úÖ\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Third weak learner (Tree #3)\n",
    "\n",
    "Since predictions are already perfect, this tree doesn‚Äôt change anything (errors are all zero).\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Final Model = Tree #1 + Tree #2**\n",
    "\n",
    "* Size=1 ‚Üí Apple\n",
    "* Size=2 ‚Üí Apple\n",
    "* Size=3 ‚Üí Orange\n",
    "\n",
    "---\n",
    "\n",
    "üîë **Difference vs Regression case:**\n",
    "\n",
    "* In regression, weak learners predict **numerical residuals**.\n",
    "* In classification, weak learners predict **which class was misclassified**, and updates are done in terms of probabilities (log-odds)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f80b9b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# üå± Gradient Boosting Classifier ‚Äì Key Terms\n",
    "\n",
    "### 1. **Weak Learner**\n",
    "\n",
    "* A very simple model, usually a small decision tree (called a *stump*).\n",
    "* On its own, it‚Äôs ‚Äúweak‚Äù (not very accurate).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Ensemble**\n",
    "\n",
    "* The final Gradient Boosting model is not just one tree.\n",
    "* It‚Äôs a **team of many weak trees**, each one fixing mistakes of the previous.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Loss Function**\n",
    "\n",
    "* A way to measure how wrong the model is.\n",
    "* For classification, usually **log-loss** (penalizes wrong class predictions).\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Residuals (Errors)**\n",
    "\n",
    "* After making predictions, we check where the model went wrong.\n",
    "* These mistakes are turned into numbers (gradients) that the next tree tries to fix.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Gradient**\n",
    "\n",
    "* Tells us the **direction to move** to reduce errors.\n",
    "* Each new tree learns from this direction.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Learning Rate**\n",
    "\n",
    "* A small step size that controls how much each new tree contributes.\n",
    "* Small learning rate = slower learning, but usually better accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Number of Estimators**\n",
    "\n",
    "* The number of trees we add to the model.\n",
    "* More trees = better fit, but too many = risk of overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. **Tree Depth**\n",
    "\n",
    "* How deep each tree is (how many splits it can make).\n",
    "* Shallow trees (like depth=1 or 2) are common, because boosting works best with simple learners.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. **Subsampling**\n",
    "\n",
    "* Instead of using all the data for each tree, we randomly take part of it.\n",
    "* Helps prevent overfitting and adds variety.\n",
    "\n",
    "---\n",
    "\n",
    "### 10. **Regularization**\n",
    "\n",
    "* Tricks to stop the model from memorizing the data (overfitting).\n",
    "* Examples:\n",
    "\n",
    "  * Keep trees small.\n",
    "  * Use fewer features per tree.\n",
    "  * Use a smaller learning rate.\n",
    "\n",
    "---\n",
    "\n",
    "### 11. **Additive Model**\n",
    "\n",
    "* Gradient boosting adds trees **one by one**, each improving the previous result:\n",
    "\n",
    "  $$\n",
    "  \\text{New model} = \\text{Old model} + \\text{Small correction}\n",
    "  $$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
