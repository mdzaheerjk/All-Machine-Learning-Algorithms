{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7591e88a",
   "metadata": {},
   "source": [
    "# Gradient Boosting regressor with Three Weak learners"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7d76d0",
   "metadata": {},
   "source": [
    "### Tiny Example (3 weak learners)\n",
    "\n",
    "**Data:**\n",
    "\n",
    "* (Size=1 ‚Üí Price=100)\n",
    "* (Size=2 ‚Üí Price=200)\n",
    "* (Size=3 ‚Üí Price=300)\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 1: First weak learner (Tree #1)\n",
    "\n",
    "Predicts the **average = 200** for everything.\n",
    "\n",
    "* Size=1 ‚Üí Predict 200 (error = -100)\n",
    "* Size=2 ‚Üí Predict 200 (error = 0)\n",
    "* Size=3 ‚Üí Predict 200 (error = +100)\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 2: Second weak learner (Tree #2)\n",
    "\n",
    "Learns to **predict the errors** from step 1:\n",
    "\n",
    "* If Size=1 ‚Üí predict -100\n",
    "* If Size=2 ‚Üí predict 0\n",
    "* If Size=3 ‚Üí predict +100\n",
    "\n",
    "Add these corrections to Tree #1:\n",
    "\n",
    "* Size=1 ‚Üí 200 + (-100) = 100 ‚úÖ\n",
    "* Size=2 ‚Üí 200 + (0) = 200 ‚úÖ\n",
    "* Size=3 ‚Üí 200 + (+100) = 300 ‚úÖ\n",
    "\n",
    "Now predictions are perfect.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 3: Third weak learner (Tree #3)\n",
    "\n",
    "No errors left (all 0), so Tree #3 does nothing.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Final Model = Tree #1 + Tree #2**\n",
    "Predictions exactly match the true prices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb66ed6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# üå± Gradient Boosting Regressor ‚Äì Key Terms\n",
    "\n",
    "### 1. **Weak Learner**\n",
    "\n",
    "* A small, simple model (usually a shallow decision tree).\n",
    "* On its own, it‚Äôs not very accurate.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Ensemble**\n",
    "\n",
    "* The final model is a **collection of many weak trees**.\n",
    "* Each new tree improves on the mistakes of the ones before it.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Loss Function**\n",
    "\n",
    "* A way to measure prediction error.\n",
    "* Common ones for regression:\n",
    "\n",
    "  * **Mean Squared Error (MSE):** penalizes big errors more.\n",
    "  * **Mean Absolute Error (MAE):** looks at average distance between prediction and true value.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Residuals (Errors)**\n",
    "\n",
    "* The difference between the true value and the model‚Äôs prediction:\n",
    "\n",
    "  $$\n",
    "  \\text{Residual} = y - \\hat{y}\n",
    "  $$\n",
    "* Each new tree is trained to predict these residuals (the mistakes).\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Gradient**\n",
    "\n",
    "* A more general way of saying ‚Äúdirection of error‚Äù (especially when using other loss functions).\n",
    "* The new tree learns to follow this direction to reduce mistakes.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Learning Rate**\n",
    "\n",
    "* A small multiplier that controls how much each new tree affects the model.\n",
    "* Small learning rate = slower progress, but usually more accurate in the long run.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Number of Estimators**\n",
    "\n",
    "* The number of trees added.\n",
    "* More trees = better fit, but too many = risk of overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. **Tree Depth**\n",
    "\n",
    "* Controls how complex each weak tree is.\n",
    "* Shallow trees (depth=1‚Äì3) are common, because they focus on small corrections.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. **Subsampling**\n",
    "\n",
    "* Instead of using all data for every tree, we use only part of it.\n",
    "* This makes the model more robust and less likely to overfit.\n",
    "\n",
    "---\n",
    "\n",
    "### 10. **Regularization**\n",
    "\n",
    "* Ways to keep the model simpler and prevent overfitting:\n",
    "\n",
    "  * Limit tree depth.\n",
    "  * Use fewer features per tree.\n",
    "  * Use a small learning rate.\n",
    "\n",
    "---\n",
    "\n",
    "### 11. **Additive Model**\n",
    "\n",
    "* Gradient boosting builds the model step by step:\n",
    "\n",
    "  $$\n",
    "  \\text{New prediction} = \\text{Old prediction} + \\text{Small correction}\n",
    "  $$\n",
    "* Over many steps, predictions get closer to the true values."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
